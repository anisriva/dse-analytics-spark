{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from pyspark.sql import SparkSession, functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StructType, \n",
    "                               IntegerType, StringType,\n",
    "                              FloatType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"file:///\"+environ['DATA_LAKE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "            StructField(name=\"station_id\", dataType=StringType(), nullable=False),\n",
    "            StructField(name=\"entitiy_id\", dataType=IntegerType(), nullable=False),\n",
    "            StructField(name=\"temp_type\", dataType=StringType(), nullable=True),\n",
    "            StructField(name=\"temp\", dataType=FloatType(), nullable=True),\n",
    "            StructField(name=\"col_5\", dataType=StringType(), nullable=True),\n",
    "            StructField(name=\"col_6\", dataType=StringType(), nullable=True)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_stuct = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"min_max_temp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.read.schema(schema_stuct).format(\"csv\").load(file_path+\"1800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temps = temp_df.filter(temp_df.temp_type==\"TMIN\")\n",
    "max_temps = temp_df.filter(temp_df.temp_type==\"TMAX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp_station = min_temps.select(min_temps.station_id, min_temps.temp).groupBy(min_temps.station_id).min(\"temp\").withColumnRenamed(\"min(temp)\",\"min_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp_per_station = min_temp_station \\\n",
    ".withColumn(\"temperature\", f.round(f.col(\"min_temp\")*0.1*(9.0/5.0)+32.0,2)) \\\n",
    ".select(\"station_id\",\"temperature\") \\\n",
    ".sort(\"temperature\") \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "for row in min_temp_per_station:\n",
    "    print(row[0] + \"\\t{:.2f}F\".format(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "668a0c9365313c23e463277e6aba067c287d5f8af493a075d87a49f0c4e66349"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
