{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Window Functions\n",
    "Pyspark window functions are useful when you want to examine relationships within groups of data rather than between groups of data (as for groupBy)\n",
    "\n",
    "To use them you start by defining a window function then select a separate function or set of functions to operate within that window\n",
    "\n",
    "NB- this workbook is designed to work on Databricks Community Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = {'partition': ['a','a', 'a', 'a', 'b', 'b', 'b', 'c', 'c',],\n",
    "           'col_1': [1,1,1,1,2,2,2,3,3,], \n",
    "           'aggregation': [1,2,3,4,5,6,7,8,9,],\n",
    "           'ranking': [4,3,2,1,1,1,3,1,5,],\n",
    "           'lagging': [9,8,7,6,5,4,3,2,1,],\n",
    "           'cumulative': [1,2,4,6,1,1,1,20,30,],\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+-------+-------+----------+\n",
      "|partition|col_1|aggregation|ranking|lagging|cumulative|\n",
      "+---------+-----+-----------+-------+-------+----------+\n",
      "|        a|    1|          1|      4|      9|         1|\n",
      "|        a|    1|          2|      3|      8|         2|\n",
      "|        a|    1|          3|      2|      7|         4|\n",
      "|        a|    1|          4|      1|      6|         6|\n",
      "|        b|    2|          5|      1|      5|         1|\n",
      "|        b|    2|          6|      1|      4|         1|\n",
      "|        b|    2|          7|      3|      3|         1|\n",
      "|        c|    3|          8|      1|      2|        20|\n",
      "|        c|    3|          9|      5|      1|        30|\n",
      "+---------+-----+-----------+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pandas = pd.DataFrame(df_data)\n",
    "# create spark dataframe\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple aggregation functions\n",
    "we can use the standard group by aggregations with window functions. These functions use the simplest form of window which just defines grouping.\n",
    "Aggregation functions use the simplest form of window which just defines grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy('partition')\n",
    "df_aggregations = df.select(\n",
    "    'partition','aggregation'\n",
    ").withColumn('aggregate_sum', fn.sum('aggregation').over(windowSpec)\n",
    ").withColumn('aggregate_avg', fn.avg('aggregation').over(windowSpec)\n",
    ").withColumn('aggregate_min', fn.min('aggregation').over(windowSpec)\n",
    ").withColumn('aggregate_max', fn.max('aggregation').over(windowSpec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+-------------+-------------+-------------+\n",
      "|partition|aggregation|aggregate_sum|aggregate_avg|aggregate_min|aggregate_max|\n",
      "+---------+-----------+-------------+-------------+-------------+-------------+\n",
      "|        a|          3|           10|          2.5|            1|            4|\n",
      "|        a|          4|           10|          2.5|            1|            4|\n",
      "|        a|          1|           10|          2.5|            1|            4|\n",
      "|        a|          2|           10|          2.5|            1|            4|\n",
      "|        b|          5|           18|          6.0|            5|            7|\n",
      "|        b|          6|           18|          6.0|            5|            7|\n",
      "|        b|          7|           18|          6.0|            5|            7|\n",
      "|        c|          8|           17|          8.5|            8|            9|\n",
      "|        c|          9|           17|          8.5|            8|            9|\n",
      "+---------+-----------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aggregations.orderBy('partition').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row wise ordering and ranking functions\n",
    "We can also use window funtions to order and rank data. These functions add an element to the definition of the window which defines both grouping AND ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_window = Window.partitionBy('partition').orderBy('ranking')\n",
    "\n",
    "df_rank = df.select('partition', 'aggregation', 'ranking'\n",
    ").withColumn('ranking_row_num', fn.row_number().over(rank_window)\n",
    ").withColumn('ranking_rank', fn.rank().over(rank_window)\n",
    ").withColumn('ranking_dense_rank', fn.dense_rank().over(rank_window)\n",
    ").withColumn('ranking_per_rank', fn.percent_rank().over(rank_window)\n",
    ").withColumn('ranking_ntile_rank', fn.ntile(2).over(rank_window)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+---------------+------------+------------------+------------------+------------------+\n",
      "|partition|aggregation|ranking|ranking_row_num|ranking_rank|ranking_dense_rank|  ranking_per_rank|ranking_ntile_rank|\n",
      "+---------+-----------+-------+---------------+------------+------------------+------------------+------------------+\n",
      "|        c|          8|      1|              1|           1|                 1|               0.0|                 1|\n",
      "|        c|          9|      5|              2|           2|                 2|               1.0|                 2|\n",
      "|        b|          5|      1|              1|           1|                 1|               0.0|                 1|\n",
      "|        b|          6|      1|              2|           1|                 1|               0.0|                 1|\n",
      "|        b|          7|      3|              3|           3|                 2|               1.0|                 2|\n",
      "|        a|          4|      1|              1|           1|                 1|               0.0|                 1|\n",
      "|        a|          3|      2|              2|           2|                 2|0.3333333333333333|                 1|\n",
      "|        a|          2|      3|              3|           3|                 3|0.6666666666666666|                 2|\n",
      "|        a|          1|      4|              4|           4|                 4|               1.0|                 2|\n",
      "+---------+-----------+-------+---------------+------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lagged columns\n",
    "If we want to conduct operations like calculating the difference between subsequent operations in a group, we can use window functions to create the lagged values we require to perform the calculation. Where there is no preceding lag value, a null entry will be inserted not a zero.\n",
    "\n",
    "The inverse of lag is lead. Effectively fn.lag(n) == fn.lead(-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+-------------+--------------+--------------+------------------+\n",
      "|partition|aggregation|lagging|lagging_lag_1|lagging_lag_m1|lagging_lead_1|difference_between|\n",
      "+---------+-----------+-------+-------------+--------------+--------------+------------------+\n",
      "|        c|          9|      1|         null|             2|             2|              null|\n",
      "|        c|          8|      2|            1|          null|          null|                 1|\n",
      "|        b|          7|      3|         null|             4|             4|              null|\n",
      "|        b|          6|      4|            3|             5|             5|                 1|\n",
      "|        b|          5|      5|            4|          null|          null|                 1|\n",
      "|        a|          4|      6|         null|             7|             7|              null|\n",
      "|        a|          3|      7|            6|             8|             8|                 1|\n",
      "|        a|          2|      8|            7|             9|             9|                 1|\n",
      "|        a|          1|      9|            8|          null|          null|                 1|\n",
      "+---------+-----------+-------+-------------+--------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lag_window = Window.partitionBy('partition').orderBy('lagging')\n",
    "\n",
    "df_lag = df.select('partition', 'aggregation', 'lagging'\n",
    ").withColumn('lagging_lag_1', fn.lag('lagging', 1).over(lag_window)\n",
    ").withColumn('lagging_lag_m1', fn.lag('lagging', -1).over(lag_window)\n",
    ").withColumn('lagging_lead_1', fn.lead('lagging', 1).over(lag_window)\n",
    ").withColumn('difference_between', fn.col('lagging') - fn.lag('lagging', 1).over(lag_window)\n",
    ")\n",
    "\n",
    "df_lag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Calculations (Running totals and averages)\n",
    "There are often good reasons to want to create a running total or running average column. In some cases we might want running totals for subsets of data. Window functions can be useful for that sort of thing.\n",
    "\n",
    "In order to calculate such things we need to add yet another element to the window. Now we account for partition, order and which rows should be covered by the function. This can be done in two ways we can use rangeBetween to define how similar values in the window must be to be considered, or we can use rowsBetween to define how many rows should be considered. The current row is considered row zero, the following rows are numbered positively and the preceding rows negatively. For cumulative calculations you can define \"all previous rows\" with Window.unboundedPreceding and \"all following rows\" with Window.unboundedFolowing\n",
    "\n",
    "Note that the window may vary in size as it progresses over the rows since at the start and end part of the window may \"extend past\" the existing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n",
      "|partition|cumulative|    cumulative_avg|\n",
      "+---------+----------+------------------+\n",
      "|        c|        20|              25.0|\n",
      "|        c|        30|              25.0|\n",
      "|        b|         1|               1.0|\n",
      "|        b|         1|               1.0|\n",
      "|        b|         1|               1.0|\n",
      "|        a|         1|               1.5|\n",
      "|        a|         2|2.3333333333333335|\n",
      "|        a|         4|               4.0|\n",
      "|        a|         6|               5.0|\n",
      "+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cumulative_window = Window.partitionBy('partition').rowsBetween(-1,1)\n",
    "\n",
    "df_cumulative_avg = df.select('partition', 'cumulative'\n",
    ").withColumn('cumulative_avg', fn.avg('cumulative').over(cumulative_window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Window.unboundedPreceding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|partition|cumulative|cumulative_sum|\n",
      "+---------+----------+--------------+\n",
      "|        c|        20|            20|\n",
      "|        c|        30|            50|\n",
      "|        b|         1|             1|\n",
      "|        b|         1|             2|\n",
      "|        b|         1|             3|\n",
      "|        a|         1|             1|\n",
      "|        a|         2|             3|\n",
      "|        a|         4|             7|\n",
      "|        a|         6|            13|\n",
      "+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cumulative_window_2 = Window.partitionBy('partition').orderBy('cumulative').rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_cumulative_sum = df.select('partition', 'cumulative'\n",
    ").withColumn('cumulative_sum', fn.sum('cumulative').over(cumulative_window_2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining window and different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|partition|aggregation_sum|aggregation_avg|aggregation_min|aggregation_max|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|        c|             17|            8.5|              8|              9|\n",
      "|        b|             18|            6.0|              5|              7|\n",
      "|        a|             10|            2.5|              1|              4|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|partition|aggregation_sum|aggregation_avg|aggregation_min|aggregation_max|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|        c|             17|            8.5|              8|              9|\n",
      "|        b|             18|            6.0|              5|              7|\n",
      "|        a|             10|            2.5|              1|              4|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregation_window = Window.partitionBy('partition')\n",
    "grouping_window = Window.partitionBy('partition').orderBy('aggregation')\n",
    "\n",
    "# then we can use this window function for our aggregations\n",
    "df_aggregations = df.select(\n",
    "  'partition', 'aggregation'\n",
    ").withColumn(\n",
    "  # note that we calculate row number over the grouping_window\n",
    "  'group_rank', fn.row_number().over(grouping_window) \n",
    ").withColumn(\n",
    "  # but we calculate other columns over the aggregation_window\n",
    "  'aggregation_sum', fn.sum('aggregation').over(aggregation_window),\n",
    ").withColumn(\n",
    "  'aggregation_avg', fn.avg('aggregation').over(aggregation_window),\n",
    ").withColumn(\n",
    "  'aggregation_min', fn.min('aggregation').over(aggregation_window),\n",
    ").withColumn(\n",
    "  'aggregation_max', fn.max('aggregation').over(aggregation_window),\n",
    ").where(\n",
    "  fn.col('group_rank') == 1\n",
    ").select(\n",
    "  'partition', \n",
    "  'aggregation_sum', \n",
    "  'aggregation_avg', \n",
    "  'aggregation_min', \n",
    "  'aggregation_max'\n",
    ")\n",
    "\n",
    "df_aggregations.show()\n",
    "\n",
    "# this is equivalent to the rather simpler expression below\n",
    "df_groupby = df.select(\n",
    "  'partition', 'aggregation'\n",
    ").groupBy(\n",
    "  'partition'\n",
    ").agg(\n",
    "  fn.sum('aggregation').alias('aggregation_sum'),\n",
    "  fn.avg('aggregation').alias('aggregation_avg'),\n",
    "  fn.min('aggregation').alias('aggregation_min'),\n",
    "  fn.max('aggregation').alias('aggregation_max'),\n",
    ")\n",
    "\n",
    "df_groupby.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "668a0c9365313c23e463277e6aba067c287d5f8af493a075d87a49f0c4e66349"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
