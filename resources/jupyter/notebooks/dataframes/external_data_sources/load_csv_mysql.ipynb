{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all csv files from a data store to mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from os import listdir, environ, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get all the files to be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(folder_path:str)->list:\n",
    "    '''\n",
    "    :param folder_path - path for all the files\n",
    "\n",
    "    returns list of files\n",
    "    '''\n",
    "    files = listdir(folder_path)\n",
    "    return [\"file:///\"+path.join(folder_path,file) for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to return the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(file:str , format:str=None)->DataFrame:\n",
    "    '''\n",
    "    :param file - file name\n",
    "    :param format - format of the file\n",
    "\n",
    "    returns dataframe object\n",
    "    '''\n",
    "    if not format:\n",
    "        format = path.basename(file).rsplit(\".\",1)[1]\n",
    "        \n",
    "    # print(f\"Creating dataframe for {file} of format {format}\")\n",
    "\n",
    "    return spark.read \\\n",
    "            .format(format) \\\n",
    "            .option(\"path\", file) \\\n",
    "            .option(\"header\", \"true\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to drop the dataframe into the mysql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_mysql(df:DataFrame, \n",
    "            host:str, \n",
    "            port:int,\n",
    "            db:str,\n",
    "            table:str,\n",
    "            user:str,\n",
    "            pwd:str,\n",
    "            mode:str=\"append\"\n",
    "            )->None:\n",
    "    '''\n",
    "    Loads the dataframe into the mysql table.\n",
    "    \n",
    "    :param df : required dataframe\n",
    "    :param db : database name\n",
    "    :param table : mysql target table \n",
    "    :param user : user name\n",
    "    :param pwd : password\n",
    "    :param mode : write method <append, overwrite, ignore>\n",
    "    '''\n",
    "    # print(f\"Writing the data for table {table}\")\n",
    "    \n",
    "    df.write.format('jdbc') \\\n",
    "        .options(\n",
    "            url = f\"jdbc:mysql://{host}:{port}/{db}\",\n",
    "            driver = \"com.mysql.jdbc.Driver\",\n",
    "            dbtable = table,\n",
    "            user = user,\n",
    "            password = pwd\n",
    "        ).mode(mode).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for file in files:\n",
    "        # if not 'emp_dept' in file: continue\n",
    "        load_df_mysql(\n",
    "            df = get_df(file),\n",
    "            db=dbname,\n",
    "            table=path.basename(file).rsplit(\".\")[0],\n",
    "                host=host,\n",
    "                port=port,\n",
    "                user=user,\n",
    "                pwd=password,\n",
    "                mode=write_mode\n",
    "            )\n",
    "    else:\n",
    "        print(\"Successfully loaded all the tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark session and required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"csv_loader\").getOrCreate()\n",
    "data_folder = path.join(environ['DATA_LAKE'],\"sql_files\")\n",
    "files = get_files(data_folder)\n",
    "host = 'localhost'\n",
    "port = 3306\n",
    "dbname = 'spark_tables'\n",
    "user = 'debezium'\n",
    "password = 'debezium'\n",
    "write_mode = \"overwrite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded all the tables\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "668a0c9365313c23e463277e6aba067c287d5f8af493a075d87a49f0c4e66349"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
